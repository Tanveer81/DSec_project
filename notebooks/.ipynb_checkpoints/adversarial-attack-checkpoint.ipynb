{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# This is a fork of Alex Shonenkov kernel with TTA added\n",
    "[[Train + Inference] GPU Baseline](https://www.kaggle.com/shonenkov/train-inference-gpu-baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alaska2 Baseline PyTorch\n",
    "\n",
    "Hi everyone!\n",
    "\n",
    "My name is Alex Shonenkov, I am DL/NLP/CV/TS research engineer. Especially I am in Love with NLP & DL.\n",
    "\n",
    "I would like to share with you my starter pipeline for solving this competition :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Ideas\n",
    "\n",
    "- 4 Classes\n",
    "- GroupKFold splitting\n",
    "- Class Balance\n",
    "- Flips\n",
    "- Label Smoothing\n",
    "- EfficientNetB2\n",
    "- ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# !pip install -q efficientnet_pytorch > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import sklearn\n",
    "\n",
    "SEED = 512\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupKFold splitting\n",
    "\n",
    "I think group splitting by image_name is really important for correct validation in this competition ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cover\n",
      "../alaska2-image-steganalysis/Cover/80005.jpg\n",
      "1 JMiPOD\n",
      "../alaska2-image-steganalysis/JMiPOD/80005.jpg\n",
      "2 JUNIWARD\n",
      "../alaska2-image-steganalysis/JUNIWARD/80005.jpg\n",
      "3 UERD\n",
      "../alaska2-image-steganalysis/UERD/80005.jpg\n",
      "4 SteganoGAN\n",
      "../alaska2-image-steganalysis/SteganoGAN/80005.png\n",
      "CPU times: user 2.16 s, sys: 119 ms, total: 2.28 s\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for label, kind in enumerate(['Cover', 'JMiPOD', 'JUNIWARD', 'UERD', 'SteganoGAN']):\n",
    "    print(label, kind)\n",
    "#     for path in glob(f'../alaska2-image-steganalysis/{kind}/*.jpg'):\n",
    "    for path in sorted(os.listdir(f'alaska2-image-steganalysis/{kind}/')):\n",
    "        path = '../alaska2-image-steganalysis/' + kind + '/' + path\n",
    "        dataset.append({\n",
    "            'kind': kind,\n",
    "            'image_name': path.split('/')[-1],\n",
    "            'label': label\n",
    "        })\n",
    "    print(path)\n",
    "random.shuffle(dataset)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "gkf = GroupKFold(n_splits=32)\n",
    "\n",
    "dataset.loc[:, 'fold'] = 0\n",
    "for fold_number, (train_index, val_index) in enumerate(gkf.split(X=dataset.index, y=dataset['label'], groups=dataset['image_name'])):\n",
    "    dataset.loc[dataset.iloc[val_index].index, 'fold'] = fold_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/hannan/alaska\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case original dataframe with splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv('../input/alaska2-public-baseline/groupkfold_by_shonenkov.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Augs: Flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose([\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_PATH = 'alaska2-image-steganalysis'\n",
    "\n",
    "def onehot(size, target):\n",
    "    vec = torch.zeros(size, dtype=torch.float32)\n",
    "    vec[target] = 1.\n",
    "    return vec\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, kinds, image_names, labels, transforms=None):\n",
    "        super().__init__()\n",
    "        self.kinds = kinds\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        kind, image_name, label = self.kinds[index], self.image_names[index], self.labels[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/{kind}/{image_name}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "        target = onehot(5, label) #@Tanveer\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_names.shape[0]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return list(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_number = 0\n",
    "\n",
    "# train_dataset = DatasetRetriever(\n",
    "#     kinds=dataset[dataset['fold'] != fold_number].kind.values,\n",
    "#     image_names=dataset[dataset['fold'] != fold_number].image_name.values,\n",
    "#     labels=dataset[dataset['fold'] != fold_number].label.values,\n",
    "#     transforms=get_train_transforms(),\n",
    "# )\n",
    "\n",
    "# validation_dataset = DatasetRetriever(\n",
    "#     kinds=dataset[dataset['fold'] == fold_number].kind.values,\n",
    "#     image_names=dataset[dataset['fold'] == fold_number].image_name.values,\n",
    "#     labels=dataset[dataset['fold'] == fold_number].label.values,\n",
    "#     transforms=get_valid_transforms(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image, target = train_dataset[0]\n",
    "# numpy_image = image.permute(1,2,0).cpu().numpy()\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "    \n",
    "# ax.set_axis_off()\n",
    "# ax.imshow(numpy_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "        \n",
    "def alaska_weighted_auc(y_true, y_valid):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/anokas/weighted-auc-metric-updated\n",
    "    \"\"\"\n",
    "    tpr_thresholds = [0.0, 0.4, 1.0]\n",
    "    weights = [2, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_valid, pos_label=1)\n",
    "\n",
    "    # size of subsets\n",
    "    areas = np.array(tpr_thresholds[1:]) - np.array(tpr_thresholds[:-1])\n",
    "\n",
    "    # The total area is normalized by the sum of weights such that the final weighted AUC is between 0 and 1.\n",
    "    normalization = np.dot(areas, weights)\n",
    "\n",
    "    competition_metric = 0\n",
    "    for idx, weight in enumerate(weights):\n",
    "        y_min = tpr_thresholds[idx]\n",
    "        y_max = tpr_thresholds[idx + 1]\n",
    "        mask = (y_min < tpr) & (tpr < y_max)\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        x_padding = np.linspace(fpr[mask][-1], 1, 100)\n",
    "\n",
    "        x = np.concatenate([fpr[mask], x_padding])\n",
    "        y = np.concatenate([tpr[mask], [y_max] * len(x_padding)])\n",
    "        y = y - y_min  # normalize such that curve starts at y=0\n",
    "        score = metrics.auc(x, y)\n",
    "        submetric = score * weight\n",
    "        best_subscore = (y_max - y_min) * weight\n",
    "        competition_metric += submetric\n",
    "\n",
    "    return competition_metric / normalization\n",
    "        \n",
    "class RocAucMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_true = np.array([0,1])\n",
    "        self.y_pred = np.array([0.5,0.5])\n",
    "        self.score = 0\n",
    "\n",
    "    def update(self, y_true, y_pred):\n",
    "        y_true = y_true.cpu().numpy().argmax(axis=1).clip(min=0, max=1).astype(int)\n",
    "        y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "        self.y_true = np.hstack((self.y_true, y_true))\n",
    "        self.y_pred = np.hstack((self.y_pred, y_pred))\n",
    "        self.score = alaska_weighted_auc(self.y_true, self.y_pred)\n",
    "    \n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing = 0.05):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        if self.training:\n",
    "            x = x.float()\n",
    "            target = target.float()\n",
    "            logprobs = torch.nn.functional.log_softmax(x, dim = -1)\n",
    "\n",
    "            nll_loss = -logprobs * target\n",
    "            nll_loss = nll_loss.sum(-1)\n",
    "    \n",
    "            smooth_loss = -logprobs.mean(dim=-1)\n",
    "\n",
    "            loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return torch.nn.functional.cross_entropy(x, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Fitter:\n",
    "    \n",
    "    def __init__(self, model, device, config):\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.base_dir = './'\n",
    "        self.log_path = f'{self.base_dir}/log.txt'\n",
    "        self.best_summary_loss = 10**5\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ] \n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "        self.criterion = LabelSmoothing().to(self.device)\n",
    "        self.log(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, final_scores = self.train_one_epoch(train_loader)\n",
    "\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            self.save(f'{self.base_dir}/last-checkpoint.bin')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, final_scores = self.validation(validation_loader)\n",
    "\n",
    "            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                self.model.eval()\n",
    "                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=summary_loss.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        summary_loss = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Val Step {step}/{len(val_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                targets = targets.to(self.device).float()\n",
    "                batch_size = images.shape[0]\n",
    "                images = images.to(self.device).float()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                final_scores.update(targets, outputs)\n",
    "                summary_loss.update(loss.detach().item(), batch_size)\n",
    "\n",
    "        return summary_loss, final_scores\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            \n",
    "            targets = targets.to(self.device).float()\n",
    "            images = images.to(self.device).float()\n",
    "            batch_size = images.shape[0]\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            final_scores.update(targets, outputs)\n",
    "            summary_loss.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        return summary_loss, final_scores\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_summary_loss': self.best_summary_loss,\n",
    "            'epoch': self.epoch,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
    "        self.epoch = checkpoint['epoch'] + 1\n",
    "        \n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b2\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "def get_net():\n",
    "    net = EfficientNet.from_pretrained('efficientnet-b2')\n",
    "    net._fc = nn.Linear(in_features=1408, out_features=4, bias=True)\n",
    "    return net\n",
    "\n",
    "net = get_net().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    num_workers = 4\n",
    "    batch_size = 22\n",
    "    n_epochs = 3\n",
    "    lr = 0.001\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  # do scheduler.step after optimizer.step\n",
    "    validation_scheduler = True  # do scheduler.step after validation stage loss\n",
    "\n",
    "#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n",
    "#     scheduler_params = dict(\n",
    "#         max_lr=0.001,\n",
    "#         epochs=n_epochs,\n",
    "#         steps_per_epoch=int(len(train_dataset) / batch_size),\n",
    "#         pct_start=0.1,\n",
    "#         anneal_strategy='cos', \n",
    "#         final_div_factor=10**5\n",
    "#     )\n",
    "    \n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='min',\n",
    "        factor=0.6,\n",
    "        patience=1,\n",
    "        verbose=False, \n",
    "        threshold=0.001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0, \n",
    "        min_lr=1e-9,\n",
    "        eps=1e-09\n",
    "    )\n",
    "    # --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Balance \"on fly\" from [@CatalystTeam](https://github.com/catalyst-team/catalyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.data.sampler import BalanceClassSampler\n",
    "\n",
    "def run_training():\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, \n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        shuffle=False,\n",
    "        sampler=SequentialSampler(validation_dataset),\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    fitter.load(\"last-checkpoint.bin\")\n",
    "    fitter.fit(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "I have used 1xV100 for training model, in kaggle kernel it works also. You can make fork and check it, but I would like to share with you my logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 14 16:27:48 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:65:00.0 Off |                  N/A |\r\n",
      "| 23%   34C    P2    70W / 250W |    531MiB / 11178MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:B3:00.0 Off |                  N/A |\r\n",
      "| 23%   23C    P8     7W / 250W |     10MiB / 11178MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     32657      C   .../miniconda3/envs/contrastive/bin/python   521MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# file = open('../input/alaska2-checkpoint/log.txt', 'r')\n",
    "# for line in file.readlines():\n",
    "#     print(line[:-1])\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('best-checkpoint-143epoch.bin')\n",
    "net.load_state_dict(checkpoint['model_state_dict']);\n",
    "net.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In checkpoint you can find states for optimizer and scheduler if you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'best_summary_loss', 'epoch'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Time Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_transforms(mode):\n",
    "    if mode == 0:\n",
    "        return A.Compose([\n",
    "                A.Resize(height=512, width=512, p=1.0),\n",
    "                ToTensorV2(p=1.0),\n",
    "            ], p=1.0)\n",
    "    elif mode == 1:\n",
    "        return A.Compose([\n",
    "                A.HorizontalFlip(p=1),\n",
    "                A.Resize(height=512, width=512, p=1.0),\n",
    "                ToTensorV2(p=1.0),\n",
    "            ], p=1.0)    \n",
    "    elif mode == 2:\n",
    "        return A.Compose([\n",
    "                A.VerticalFlip(p=1),\n",
    "                A.Resize(height=512, width=512, p=1.0),\n",
    "                ToTensorV2(p=1.0),\n",
    "            ], p=1.0)\n",
    "    else:\n",
    "        return A.Compose([\n",
    "                A.HorizontalFlip(p=1),\n",
    "                A.VerticalFlip(p=1),\n",
    "                A.Resize(height=512, width=512, p=1.0),\n",
    "                ToTensorV2(p=1.0),\n",
    "            ], p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSubmissionRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, image_names, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_names = image_names\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_name = self.image_names[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/Test/{image_name}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "\n",
    "        return image_name, image\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_names.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# for mode in range(0, 4):\n",
    "#     dataset = DatasetSubmissionRetriever(\n",
    "#         image_names=np.array([path.split('/')[-1] for path in glob('alaska2-image-steganalysis/Test/*.jpg')]),\n",
    "#         transforms=get_test_transforms(mode),\n",
    "#     )\n",
    "\n",
    "\n",
    "#     data_loader = DataLoader(\n",
    "#         dataset,\n",
    "#         batch_size=8,\n",
    "#         shuffle=False,\n",
    "#         num_workers=2,\n",
    "#         drop_last=False,\n",
    "#     )\n",
    "    \n",
    "#     result = {'Id': [], 'Label': []}\n",
    "#     for step, (image_names, images) in enumerate(data_loader):\n",
    "#         print(step, end='\\r')\n",
    "\n",
    "#         y_pred = net(images.cuda())\n",
    "#         y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "\n",
    "#         result['Id'].extend(image_names)\n",
    "#         result['Label'].extend(y_pred)\n",
    "        \n",
    "#     results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Tanveer\n",
    "class CustomDatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, kinds, image_names, labels, transforms=None):\n",
    "        super().__init__()\n",
    "        self.kinds = kinds\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        kind, image_name, label = self.kinds[index], self.image_names[index], self.labels[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/{kind}/{image_name}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "        target = onehot(5, label) #@ Tanveer\n",
    "        return image_name, image, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_names.shape[0]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return list(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ssasdasd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-acd15957740e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mssasdasd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ssasdasd' is not defined"
     ]
    }
   ],
   "source": [
    "ssasdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Tanveer\n",
    "results = []\n",
    "for mode in range(0, 4):\n",
    "#     dataset = DatasetSubmissionRetriever(\n",
    "#         image_names=np.array([path.split('/')[-1] for path in glob('alaska2-image-steganalysis/Test/*.jpg')]),\n",
    "#         transforms=get_test_transforms(mode),\n",
    "#     )\n",
    "\n",
    "    val_dataset = CustomDatasetRetriever(\n",
    "        kinds=dataset[dataset['fold'] == fold_number].kind.values,\n",
    "        image_names=dataset[dataset['fold'] == fold_number].image_name.values,\n",
    "        labels=dataset[dataset['fold'] == fold_number].label.values,\n",
    "        transforms=get_test_transforms(mode),\n",
    "    )\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    result = {'Id': [], 'Label': [], 'Target': []}\n",
    "    for step, (image_names, images, targets) in enumerate(data_loader):\n",
    "        print(step, end='\\r')\n",
    "\n",
    "        y_pred = net(images.cuda())\n",
    "        y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "\n",
    "        result['Id'].extend(image_names)\n",
    "        result['Label'].extend(y_pred)\n",
    "        result['Target'].extend(torch.argmax(targets, dim=1).tolist())\n",
    "        \n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "result = {'Id': [], 'Label': []}\n",
    "for step, (image_names, images) in enumerate(data_loader):\n",
    "    print(step, end='\\r')\n",
    "    \n",
    "    y_pred = net(images.cuda())\n",
    "    y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "    \n",
    "    result['Id'].extend(image_names)\n",
    "    result['Label'].extend(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = []\n",
    "for mode in range(0,4):\n",
    "    submission = pd.DataFrame(results[mode])\n",
    "    submissions.append(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in range(0,4):\n",
    "    submissions[mode].to_csv(f'submission_{mode}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions[0]['Label'] = (submissions[0]['Label']*3 + submissions[1]['Label'] + submissions[2]['Label'] + submissions[3]['Label']) / 6\n",
    "submissions[0].to_csv(f'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torchattacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    sampler=SequentialSampler(validation_dataset),\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (images, targets) in enumerate(val_loader):\n",
    "    print(images.shape, targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.tensor([1]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape, targets.shape, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=torch.argmax(targets, axis=1).long().cuda()\n",
    "targets.shape, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchattacks\n",
    "atk = torchattacks.FGSM(net, eps=0.3)\n",
    "adversarial_images = atk(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[1].permute(1, 2, 0).cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adversarial_images[1].permute(1, 2, 0).cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(adversarial_images),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(images.cuda()),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atks = [torchattacks.FGSM(net, eps=8/255),\n",
    "        torchattacks.BIM(net, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.CW(net, c=1, kappa=0, steps=1000, lr=0.01),\n",
    "        torchattacks.RFGSM(net, eps=8/255, alpha=4/255, steps=1),\n",
    "        torchattacks.PGD(net, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.FFGSM(net, eps=8/255, alpha=12/255),\n",
    "        torchattacks.MIFGSM(net, eps=8/255, decay=1.0, steps=5),\n",
    "        torchattacks.TPGD(net, eps=8/255, alpha=2/255, steps=7),\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for atk in atks :\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    print(atk)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "#     for images, labels in data_loader:\n",
    "        \n",
    "    start = time.time()\n",
    "    \n",
    "    target_map_function = lambda images, labels: labels.fill_(0)\n",
    "    # atk.set_attack_mode(\"targeted\", target_map_function=target_map_function)\n",
    "    # or\n",
    "    atk.set_targeted_mode(target_map_function=target_map_function)\n",
    "    \n",
    "    adv_images = atk(images, targets)\n",
    "#     targets = targets.to(device)\n",
    "    outputs = net(adv_images)\n",
    "\n",
    "    _, pre = torch.max(outputs.data, 1)\n",
    "\n",
    "    total += 1\n",
    "    correct += (pre == targets).sum()\n",
    "\n",
    "#     imshow(torchvision.utils.make_grid(adv_images.cpu().data, normalize=True), [imagnet_data.classes[i] for i in pre])\n",
    "    print(f\"pre: {pre}\")\n",
    "    print(f\"targets: {targets}\")\n",
    "    plt.imshow(adv_images[0].permute(1, 2, 0).cpu())\n",
    "    plt.show()\n",
    "\n",
    "    print('Total elapsed time (sec) : %.2f' % (time.time() - start))\n",
    "    print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SteganoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread('alaska2-image-steganalysis/SteganoGAN/00005.png', cv2.IMREAD_COLOR)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "image /= 255.0\n",
    "sample = {'image': image}\n",
    "sample = get_valid_transforms()(**sample)\n",
    "image = sample['image']\n",
    "image = image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(image.cuda())\n",
    "torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([0]).cuda()\n",
    "image.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for atk in atks :\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    print(atk)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "#     for images, labels in data_loader:\n",
    "        \n",
    "    start = time.time()\n",
    "    target_map_function = lambda images, labels: labels.fill_(0)\n",
    "    # atk.set_attack_mode(\"targeted\", target_map_function=target_map_function)\n",
    "    # or\n",
    "    atk.set_targeted_mode(target_map_function=target_map_function)\n",
    "    adv_images = atk(image, targets)\n",
    "#     targets = targets.to(device)\n",
    "    outputs = net(adv_images)\n",
    "\n",
    "    _, pre = torch.max(outputs.data, 1)\n",
    "\n",
    "    total += 1\n",
    "    correct += (pre == targets).sum()\n",
    "\n",
    "#     imshow(torchvision.utils.make_grid(adv_images.cpu().data, normalize=True), [imagnet_data.classes[i] for i in pre])\n",
    "    print(f\"pre: {pre}\")\n",
    "    print(f\"targets: {targets}\")\n",
    "#     plt.imshow(adv_images[0].permute(1, 2, 0).cpu())\n",
    "#     plt.show()\n",
    "\n",
    "#     print('Total elapsed time (sec) : %.2f' % (time.time() - start))\n",
    "#     print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk.attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGSM\n",
      "BIM\n",
      "CW\n",
      "RFGSM\n",
      "PGD\n",
      "FFGSM\n",
      "MIFGSM\n",
      "0\r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for dimension 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5e8fd3e324f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0matk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0matks\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0matk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'TPGD'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m              \u001b[0madv_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m              \u001b[0madv_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.6/site-packages/torchattacks/attack.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.6/site-packages/torchattacks/attacks/cw.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mf_Loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL2_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mf_Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/contrastive/lib/python3.6/site-packages/torchattacks/attacks/cw.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, outputs, labels)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# f-function in the paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mone_hot_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mone_hot_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 4"
     ]
    }
   ],
   "source": [
    "#@ Tanveer\n",
    "import torchattacks\n",
    "\n",
    "atks = [torchattacks.FGSM(net, eps=8/255),\n",
    "        torchattacks.BIM(net, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.CW(net, c=1, kappa=0, steps=1000, lr=0.01),\n",
    "        torchattacks.RFGSM(net, eps=8/255, alpha=4/255, steps=1),\n",
    "        torchattacks.PGD(net, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.FFGSM(net, eps=8/255, alpha=12/255),\n",
    "        torchattacks.MIFGSM(net, eps=8/255, steps=5, decay=1.0),\n",
    "        torchattacks.TPGD(net, eps=8/255, alpha=2/255, steps=7),\n",
    "       ]\n",
    "\n",
    "val_dataset = CustomDatasetRetriever(\n",
    "    kinds=dataset[dataset['fold'] == fold_number].kind.values,\n",
    "    image_names=dataset[dataset['fold'] == fold_number].image_name.values,\n",
    "    labels=dataset[dataset['fold'] == fold_number].label.values,\n",
    "    transforms=get_valid_transforms(),\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "target_map_function = lambda images, labels: (labels==0).long()*np.random.randint(1,5)\n",
    "for atk in atks[:-1]:\n",
    "    print(atk.attack)\n",
    "    atk.set_targeted_mode(target_map_function=target_map_function)\n",
    "    \n",
    "result = {'Id': [], 'Truth': [], 'Prediction': [],}\n",
    "for step, (image_names, images, targets) in enumerate(data_loader):\n",
    "    print(step, end='\\r')\n",
    "    start = time.time()\n",
    "\n",
    "    targets = torch.argmax(targets, dim=1)\n",
    "    y_pred = net(images.cuda())\n",
    "    y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "\n",
    "    for atk in atks :\n",
    "        if atk.attack != 'TPGD':\n",
    "             adv_images = atk(images, (targets==0).long()*np.random.randint(1,5))\n",
    "        else:\n",
    "             adv_images = atk(images, targets)\n",
    "        y_pred_attk = net(adv_images)\n",
    "        y_pred_attk = 1 - nn.functional.softmax(y_pred_attk, dim=1).data.cpu().numpy()[:,0]\n",
    "        try:\n",
    "            result[f'{atk.attack}_prediction'].extend(y_pred_attk)\n",
    "        except KeyError:\n",
    "            result[f'{atk.attack}_prediction'] = y_pred_attk\n",
    "        \n",
    "\n",
    "    result['Id'].extend(image_names)\n",
    "    result['Prediction'].extend(y_pred)\n",
    "    result['Truth'].extend(targets.tolist())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning\n",
    "    All algorithms: Juniward, ., ., steganogan\n",
    "    All adv attks......\n",
    "    1. Create Stego Images\n",
    "    2. Get predictions for all algorithms\n",
    "    3. All Adv. Attack for all algorithms \n",
    "    4. Fine Tuning Kaggle Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(targets==0).long()*np.random.randint(1,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
