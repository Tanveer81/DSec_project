{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# This is a fork of Alex Shonenkov kernel with TTA added\n",
    "[[Train + Inference] GPU Baseline](https://www.kaggle.com/shonenkov/train-inference-gpu-baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alaska2 Baseline PyTorch\n",
    "\n",
    "Hi everyone!\n",
    "\n",
    "My name is Alex Shonenkov, I am DL/NLP/CV/TS research engineer. Especially I am in Love with NLP & DL.\n",
    "\n",
    "I would like to share with you my starter pipeline for solving this competition :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Ideas\n",
    "\n",
    "- 4 Classes\n",
    "- GroupKFold splitting\n",
    "- Class Balance\n",
    "- Flips\n",
    "- Label Smoothing\n",
    "- EfficientNetB2\n",
    "- ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# !pip install -q efficientnet_pytorch > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import sklearn\n",
    "\n",
    "SEED = 512\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupKFold splitting\n",
    "\n",
    "I think group splitting by image_name is really important for correct validation in this competition ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alaska2-image-steganalysis/SteganoGAN/80005.png\n",
      "CPU times: user 475 ms, sys: 39.4 ms, total: 514 ms\n",
      "Wall time: 510 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = []\n",
    "\n",
    "# for label, kind in enumerate(['Cover', 'JMiPOD', 'JUNIWARD', 'UERD', 'SteganoGAN']):\n",
    "#     print(label, kind)\n",
    "# #     for path in glob(f'../alaska2-image-steganalysis/{kind}/*.jpg'):\n",
    "#     for path in sorted(os.listdir(f'alaska2-image-steganalysis/{kind}/')):\n",
    "#         path = '../alaska2-image-steganalysis/' + kind + '/' + path\n",
    "#         dataset.append({\n",
    "#             'kind': kind,\n",
    "#             'image_name': path.split('/')[-1],\n",
    "#             'label': label\n",
    "#         })\n",
    "#     print(path)\n",
    "for path in sorted(os.listdir(f'alaska2-image-steganalysis/SteganoGAN/')):\n",
    "    path = 'alaska2-image-steganalysis/SteganoGAN/' + path\n",
    "    dataset.append({\n",
    "        'kind': 'SteganoGAN',\n",
    "        'image_name': path.split('/')[-1],\n",
    "        'label': 4\n",
    "    })\n",
    "print(path)\n",
    "\n",
    "random.shuffle(dataset)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "gkf = GroupKFold(n_splits=32)\n",
    "\n",
    "dataset.loc[:, 'fold'] = 0\n",
    "for fold_number, (train_index, val_index) in enumerate(gkf.split(X=dataset.index, y=dataset['label'], groups=dataset['image_name'])):\n",
    "    dataset.loc[dataset.iloc[val_index].index, 'fold'] = fold_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/hannan/alaska\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case original dataframe with splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv('../input/alaska2-public-baseline/groupkfold_by_shonenkov.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Augs: Flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose([\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_PATH = 'alaska2-image-steganalysis'\n",
    "\n",
    "def onehot(size, target):\n",
    "    vec = torch.zeros(size, dtype=torch.float32)\n",
    "    vec[target] = 1.\n",
    "    return vec\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, kinds, image_names, labels, transforms=None):\n",
    "        super().__init__()\n",
    "        self.kinds = kinds\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        kind, image_name, label = self.kinds[index], self.image_names[index], self.labels[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/{kind}/{image_name}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "        target = onehot(5, label) #@Tanveer\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_names.shape[0]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return list(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_number = 0\n",
    "\n",
    "# train_dataset = DatasetRetriever(\n",
    "#     kinds=dataset[dataset['fold'] != fold_number].kind.values,\n",
    "#     image_names=dataset[dataset['fold'] != fold_number].image_name.values,\n",
    "#     labels=dataset[dataset['fold'] != fold_number].label.values,\n",
    "#     transforms=get_train_transforms(),\n",
    "# )\n",
    "\n",
    "validation_dataset = DatasetRetriever(\n",
    "    kinds=dataset[dataset['fold'] == fold_number].kind.values,\n",
    "    image_names=dataset[dataset['fold'] == fold_number].image_name.values,\n",
    "    labels=dataset[dataset['fold'] == fold_number].label.values,\n",
    "    transforms=get_valid_transforms(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image, target = train_dataset[0]\n",
    "# numpy_image = image.permute(1,2,0).cpu().numpy()\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "    \n",
    "# ax.set_axis_off()\n",
    "# ax.imshow(numpy_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "        \n",
    "def alaska_weighted_auc(y_true, y_valid):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/anokas/weighted-auc-metric-updated\n",
    "    \"\"\"\n",
    "    tpr_thresholds = [0.0, 0.4, 1.0]\n",
    "    weights = [2, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_valid, pos_label=1)\n",
    "\n",
    "    # size of subsets\n",
    "    areas = np.array(tpr_thresholds[1:]) - np.array(tpr_thresholds[:-1])\n",
    "\n",
    "    # The total area is normalized by the sum of weights such that the final weighted AUC is between 0 and 1.\n",
    "    normalization = np.dot(areas, weights)\n",
    "\n",
    "    competition_metric = 0\n",
    "    for idx, weight in enumerate(weights):\n",
    "        y_min = tpr_thresholds[idx]\n",
    "        y_max = tpr_thresholds[idx + 1]\n",
    "        mask = (y_min < tpr) & (tpr < y_max)\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        x_padding = np.linspace(fpr[mask][-1], 1, 100)\n",
    "\n",
    "        x = np.concatenate([fpr[mask], x_padding])\n",
    "        y = np.concatenate([tpr[mask], [y_max] * len(x_padding)])\n",
    "        y = y - y_min  # normalize such that curve starts at y=0\n",
    "        score = metrics.auc(x, y)\n",
    "        submetric = score * weight\n",
    "        best_subscore = (y_max - y_min) * weight\n",
    "        competition_metric += submetric\n",
    "\n",
    "    return competition_metric / normalization\n",
    "        \n",
    "class RocAucMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_true = np.array([0,1])\n",
    "        self.y_pred = np.array([0.5,0.5])\n",
    "        self.score = 0\n",
    "\n",
    "    def update(self, y_true, y_pred):\n",
    "        y_true = y_true.cpu().numpy().argmax(axis=1).clip(min=0, max=1).astype(int)\n",
    "        y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "        self.y_true = np.hstack((self.y_true, y_true))\n",
    "        self.y_pred = np.hstack((self.y_pred, y_pred))\n",
    "        self.score = alaska_weighted_auc(self.y_true, self.y_pred)\n",
    "    \n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing = 0.05):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        if self.training:\n",
    "            x = x.float()\n",
    "            target = target.float()\n",
    "            logprobs = torch.nn.functional.log_softmax(x, dim = -1)\n",
    "\n",
    "            nll_loss = -logprobs * target\n",
    "            nll_loss = nll_loss.sum(-1)\n",
    "    \n",
    "            smooth_loss = -logprobs.mean(dim=-1)\n",
    "\n",
    "            loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return torch.nn.functional.cross_entropy(x, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Fitter:\n",
    "    \n",
    "    def __init__(self, model, device, config):\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.base_dir = './'\n",
    "        self.log_path = f'{self.base_dir}/log.txt'\n",
    "        self.best_summary_loss = 10**5\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ] \n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "        self.criterion = LabelSmoothing().to(self.device)\n",
    "        self.log(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, final_scores = self.train_one_epoch(train_loader)\n",
    "\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            self.save(f'{self.base_dir}/last-checkpoint.bin')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, final_scores = self.validation(validation_loader)\n",
    "\n",
    "            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                self.model.eval()\n",
    "                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=summary_loss.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        summary_loss = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Val Step {step}/{len(val_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                targets = targets.to(self.device).float()\n",
    "                batch_size = images.shape[0]\n",
    "                images = images.to(self.device).float()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                final_scores.update(targets, outputs)\n",
    "                summary_loss.update(loss.detach().item(), batch_size)\n",
    "\n",
    "        return summary_loss, final_scores\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            \n",
    "            targets = targets.to(self.device).float()\n",
    "            images = images.to(self.device).float()\n",
    "            batch_size = images.shape[0]\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            final_scores.update(targets, outputs)\n",
    "            summary_loss.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        return summary_loss, final_scores\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_summary_loss': self.best_summary_loss,\n",
    "            'epoch': self.epoch,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
    "        self.epoch = checkpoint['epoch'] + 1\n",
    "        \n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b2\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "def get_net():\n",
    "    net = EfficientNet.from_pretrained('efficientnet-b2')\n",
    "    net._fc = nn.Linear(in_features=1408, out_features=4, bias=True)\n",
    "    return net\n",
    "\n",
    "net = get_net().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    num_workers = 4\n",
    "    batch_size = 22\n",
    "    n_epochs = 3\n",
    "    lr = 0.001\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  # do scheduler.step after optimizer.step\n",
    "    validation_scheduler = True  # do scheduler.step after validation stage loss\n",
    "\n",
    "#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n",
    "#     scheduler_params = dict(\n",
    "#         max_lr=0.001,\n",
    "#         epochs=n_epochs,\n",
    "#         steps_per_epoch=int(len(train_dataset) / batch_size),\n",
    "#         pct_start=0.1,\n",
    "#         anneal_strategy='cos', \n",
    "#         final_div_factor=10**5\n",
    "#     )\n",
    "    \n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='min',\n",
    "        factor=0.6,\n",
    "        patience=1,\n",
    "        verbose=False, \n",
    "        threshold=0.001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0, \n",
    "        min_lr=1e-9,\n",
    "        eps=1e-09\n",
    "    )\n",
    "    # --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Balance \"on fly\" from [@CatalystTeam](https://github.com/catalyst-team/catalyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.data.sampler import BalanceClassSampler\n",
    "\n",
    "def run_training():\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, \n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        shuffle=False,\n",
    "        sampler=SequentialSampler(validation_dataset),\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    fitter.load(\"last-checkpoint.bin\")\n",
    "    fitter.fit(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "I have used 1xV100 for training model, in kaggle kernel it works also. You can make fork and check it, but I would like to share with you my logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 21 19:36:16 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:65:00.0 Off |                  N/A |\n",
      "| 23%   31C    P2    54W / 250W |    531MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:B3:00.0 Off |                  N/A |\n",
      "| 23%   24C    P8     7W / 250W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     30581      C   .../miniconda3/envs/contrastive/bin/python   521MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# file = open('../input/alaska2-checkpoint/log.txt', 'r')\n",
    "# for line in file.readlines():\n",
    "#     print(line[:-1])\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('best-checkpoint-143epoch.bin')\n",
    "net.load_state_dict(checkpoint['model_state_dict']);\n",
    "net.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In checkpoint you can find states for optimizer and scheduler if you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'best_summary_loss', 'epoch'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Time Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_transforms(mode):\n",
    "    if mode == 0:\n",
    "        return A.Compose([\n",
    "                A.Resize(height=512, width=512, p=1.0),\n",
    "                ToTensorV2(p=1.0),\n",
    "            ], p=1.0)\n",
    "    elif mode == 1:\n",
    "        return A.Compose([\n",
    "                A.HorizontalFlip(p=1),\n",
    "                A.Resize(height=512, width=512, p=1.0),\n",
    "                ToTensorV2(p=1.0),\n",
    "            ], p=1.0)    \n",
    "    elif mode == 2:\n",
    "        return A.Compose([\n",
    "                A.VerticalFlip(p=1),\n",
    "                A.Resize(height=512, width=512, p=1.0),\n",
    "                ToTensorV2(p=1.0),\n",
    "            ], p=1.0)\n",
    "    else:\n",
    "        return A.Compose([\n",
    "                A.HorizontalFlip(p=1),\n",
    "                A.VerticalFlip(p=1),\n",
    "                A.Resize(height=512, width=512, p=1.0),\n",
    "                ToTensorV2(p=1.0),\n",
    "            ], p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSubmissionRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, image_names, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_names = image_names\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_name = self.image_names[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/Test/{image_name}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "\n",
    "        return image_name, image\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_names.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# for mode in range(0, 4):\n",
    "#     dataset = DatasetSubmissionRetriever(\n",
    "#         image_names=np.array([path.split('/')[-1] for path in glob('alaska2-image-steganalysis/Test/*.jpg')]),\n",
    "#         transforms=get_test_transforms(mode),\n",
    "#     )\n",
    "\n",
    "\n",
    "#     data_loader = DataLoader(\n",
    "#         dataset,\n",
    "#         batch_size=8,\n",
    "#         shuffle=False,\n",
    "#         num_workers=2,\n",
    "#         drop_last=False,\n",
    "#     )\n",
    "    \n",
    "#     result = {'Id': [], 'Label': []}\n",
    "#     for step, (image_names, images) in enumerate(data_loader):\n",
    "#         print(step, end='\\r')\n",
    "\n",
    "#         y_pred = net(images.cuda())\n",
    "#         y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "\n",
    "#         result['Id'].extend(image_names)\n",
    "#         result['Label'].extend(y_pred)\n",
    "        \n",
    "#     results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Tanveer\n",
    "class CustomDatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, kinds, image_names, labels, transforms=None):\n",
    "        super().__init__()\n",
    "        self.kinds = kinds\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        kind, image_name, label = self.kinds[index], self.image_names[index], self.labels[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/{kind}/{image_name}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "        target = onehot(5, label) #@ Tanveer\n",
    "        return image_name, image, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_names.shape[0]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return list(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ssasdasd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-acd15957740e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mssasdasd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ssasdasd' is not defined"
     ]
    }
   ],
   "source": [
    "ssasdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\r"
     ]
    }
   ],
   "source": [
    "#@ Tanveer\n",
    "results = []\n",
    "for mode in range(0, 4):\n",
    "#     dataset = DatasetSubmissionRetriever(\n",
    "#         image_names=np.array([path.split('/')[-1] for path in glob('alaska2-image-steganalysis/Test/*.jpg')]),\n",
    "#         transforms=get_test_transforms(mode),\n",
    "#     )\n",
    "\n",
    "    val_dataset = CustomDatasetRetriever(\n",
    "        kinds=dataset[dataset['fold'] == fold_number].kind.values,\n",
    "        image_names=dataset[dataset['fold'] == fold_number].image_name.values,\n",
    "        labels=dataset[dataset['fold'] == fold_number].label.values,\n",
    "        transforms=get_test_transforms(mode),\n",
    "    )\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    result = {'Id': [], 'Label': [], 'Target': []}\n",
    "    for step, (image_names, images, targets) in enumerate(data_loader):\n",
    "        print(step, end='\\r')\n",
    "\n",
    "        y_pred = net(images.cuda())\n",
    "        y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "\n",
    "        result['Id'].extend(image_names)\n",
    "        result['Label'].extend(y_pred)\n",
    "        result['Target'].extend(torch.argmax(targets, dim=1).tolist())\n",
    "        \n",
    "        if int(step)==2:\n",
    "            break\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "result = {'Id': [], 'Label': []}\n",
    "for step, (image_names, images) in enumerate(data_loader):\n",
    "    print(step, end='\\r')\n",
    "    \n",
    "    y_pred = net(images.cuda())\n",
    "    y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "    \n",
    "    result['Id'].extend(image_names)\n",
    "    result['Label'].extend(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = []\n",
    "for mode in range(0,4):\n",
    "    submission = pd.DataFrame(results[mode])\n",
    "    submissions.append(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in range(0,4):\n",
    "    submissions[mode].to_csv(f'submission_{mode}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions[0]['Label'] = (submissions[0]['Label']*3 + submissions[1]['Label'] + submissions[2]['Label'] + submissions[3]['Label']) / 6\n",
    "submissions[0].to_csv(f'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torchattacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2343"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    sampler=SequentialSampler(validation_dataset),\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 512, 512]) torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "for step, (images, targets) in enumerate(val_loader):\n",
    "    print(images.shape, targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.tensor([1]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape, targets.shape, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=torch.argmax(targets, axis=1).long().cuda()\n",
    "targets.shape, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchattacks\n",
    "atk = torchattacks.FGSM(net, eps=0.3)\n",
    "adversarial_images = atk(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[1].permute(1, 2, 0).cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adversarial_images[1].permute(1, 2, 0).cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(adversarial_images),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(images.cuda()),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atks = [torchattacks.FGSM(net, eps=8/255),\n",
    "        torchattacks.BIM(net, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.CW(net, c=1, kappa=0, steps=1000, lr=0.01),\n",
    "        torchattacks.RFGSM(net, eps=8/255, alpha=4/255, steps=1),\n",
    "        torchattacks.PGD(net, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.FFGSM(net, eps=8/255, alpha=12/255),\n",
    "        torchattacks.MIFGSM(net, eps=8/255, decay=1.0, steps=5),\n",
    "        torchattacks.TPGD(net, eps=8/255, alpha=2/255, steps=7),\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for atk in atks :\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    print(atk)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "#     for images, labels in data_loader:\n",
    "        \n",
    "    start = time.time()\n",
    "    \n",
    "    target_map_function = lambda images, labels: labels.fill_(0)\n",
    "    # atk.set_attack_mode(\"targeted\", target_map_function=target_map_function)\n",
    "    # or\n",
    "    atk.set_targeted_mode(target_map_function=target_map_function)\n",
    "    \n",
    "    adv_images = atk(images, targets)\n",
    "#     targets = targets.to(device)\n",
    "    outputs = net(adv_images)\n",
    "\n",
    "    _, pre = torch.max(outputs.data, 1)\n",
    "\n",
    "    total += 1\n",
    "    correct += (pre == targets).sum()\n",
    "\n",
    "#     imshow(torchvision.utils.make_grid(adv_images.cpu().data, normalize=True), [imagnet_data.classes[i] for i in pre])\n",
    "    print(f\"pre: {pre}\")\n",
    "    print(f\"targets: {targets}\")\n",
    "    plt.imshow(adv_images[0].permute(1, 2, 0).cpu())\n",
    "    plt.show()\n",
    "\n",
    "    print('Total elapsed time (sec) : %.2f' % (time.time() - start))\n",
    "    print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SteganoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread('alaska2-image-steganalysis/SteganoGAN/00005.png', cv2.IMREAD_COLOR)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "image /= 255.0\n",
    "sample = {'image': image}\n",
    "sample = get_valid_transforms()(**sample)\n",
    "image = sample['image']\n",
    "image = image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(image.cuda())\n",
    "torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([0]).cuda()\n",
    "image.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for atk in atks :\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    print(atk)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "#     for images, labels in data_loader:\n",
    "        \n",
    "    start = time.time()\n",
    "    target_map_function = lambda images, labels: labels.fill_(0)\n",
    "    # atk.set_attack_mode(\"targeted\", target_map_function=target_map_function)\n",
    "    # or\n",
    "    atk.set_targeted_mode(target_map_function=target_map_function)\n",
    "    adv_images = atk(image, targets)\n",
    "#     targets = targets.to(device)\n",
    "    outputs = net(adv_images)\n",
    "\n",
    "    _, pre = torch.max(outputs.data, 1)\n",
    "\n",
    "    total += 1\n",
    "    correct += (pre == targets).sum()\n",
    "\n",
    "#     imshow(torchvision.utils.make_grid(adv_images.cpu().data, normalize=True), [imagnet_data.classes[i] for i in pre])\n",
    "    print(f\"pre: {pre}\")\n",
    "    print(f\"targets: {targets}\")\n",
    "#     plt.imshow(adv_images[0].permute(1, 2, 0).cpu())\n",
    "#     plt.show()\n",
    "\n",
    "#     print('Total elapsed time (sec) : %.2f' % (time.time() - start))\n",
    "#     print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk.attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGSM\n",
      "BIM\n",
      "CW\n",
      "RFGSM\n",
      "PGD\n",
      "FFGSM\n",
      "MIFGSM\n",
      "TPGD\n",
      "FGSM\n",
      "BIM\n",
      "CW\n",
      "RFGSM\n",
      "PGD\n",
      "FFGSM\n",
      "MIFGSM\n",
      "TPGD\n",
      "FGSM\n",
      "BIM\n",
      "CW\n",
      "RFGSM\n",
      "PGD\n",
      "FFGSM\n",
      "MIFGSM\n",
      "TPGD\n"
     ]
    }
   ],
   "source": [
    "#@ Tanveer\n",
    "import torchattacks\n",
    "results = []\n",
    "atks = [torchattacks.FGSM(net, eps=8/255),\n",
    "        torchattacks.BIM(net, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.CW(net, c=1, kappa=0, steps=1000, lr=0.01),\n",
    "        torchattacks.RFGSM(net, eps=8/255, alpha=4/255, steps=1),\n",
    "        torchattacks.PGD(net, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.FFGSM(net, eps=8/255, alpha=12/255),\n",
    "        torchattacks.MIFGSM(net, eps=8/255, decay=1.0, steps=5),\n",
    "        torchattacks.TPGD(net, eps=8/255, alpha=2/255, steps=7),\n",
    "       ]\n",
    "\n",
    "val_dataset = CustomDatasetRetriever(\n",
    "    kinds=dataset[dataset['fold'] == fold_number].kind.values,\n",
    "    image_names=dataset[dataset['fold'] == fold_number].image_name.values,\n",
    "    labels=dataset[dataset['fold'] == fold_number].label.values,\n",
    "    transforms=get_valid_transforms(),\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "target_map_function = lambda images, labels: (labels==0).long()*np.random.randint(1,4)\n",
    "for atk in atks:\n",
    "    if atk.attack != 'TPGD':\n",
    "        atk.set_targeted_mode(target_map_function=target_map_function)\n",
    "    \n",
    "for step, (image_names, images, targets) in enumerate(data_loader):\n",
    "    result = {'Id': [], 'Truth': [], 'Prediction': [],}\n",
    "    print(step, end='\\r')\n",
    "    start = time.time()\n",
    "\n",
    "    targets = torch.argmax(targets, dim=1)\n",
    "    y_pred = net(images.cuda())\n",
    "    y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "\n",
    "    for atk in atks :\n",
    "        print(atk.attack)\n",
    "        if atk.attack != 'TPGD':\n",
    "             adv_images = atk(images, targets)\n",
    "        else:\n",
    "             adv_images = atk(images, (targets==0).long()*np.random.randint(1,4))\n",
    "        y_pred_attk = net(adv_images)\n",
    "        y_pred_attk = 1 - nn.functional.softmax(y_pred_attk, dim=1).data.cpu().numpy()[:,0]\n",
    "        try:\n",
    "            result[f'{atk.attack}_prediction'].extend(y_pred_attk)\n",
    "        except KeyError:\n",
    "            result[f'{atk.attack}_prediction'] = y_pred_attk\n",
    "        \n",
    "\n",
    "    result['Id'].extend(image_names)\n",
    "    result['Prediction'].extend(y_pred)\n",
    "    result['Truth'].extend(targets.tolist())\n",
    "    \n",
    "    results.append(result)\n",
    "    if int(step)==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = []\n",
    "for i in range(len(results)):\n",
    "    submission = pd.DataFrame(results[i])\n",
    "    submissions.append(submission)\n",
    "submissions = pd.concat(submissions)\n",
    "submissions.to_csv('Evaluation/adv_attack/submission.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Truth</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>FGSM_prediction</th>\n",
       "      <th>BIM_prediction</th>\n",
       "      <th>CW_prediction</th>\n",
       "      <th>RFGSM_prediction</th>\n",
       "      <th>PGD_prediction</th>\n",
       "      <th>FFGSM_prediction</th>\n",
       "      <th>MIFGSM_prediction</th>\n",
       "      <th>TPGD_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18666.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0.891935</td>\n",
       "      <td>0.921653</td>\n",
       "      <td>0.873296</td>\n",
       "      <td>0.891935</td>\n",
       "      <td>0.759984</td>\n",
       "      <td>0.873296</td>\n",
       "      <td>0.553913</td>\n",
       "      <td>0.474206</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60731.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0.619944</td>\n",
       "      <td>0.204064</td>\n",
       "      <td>0.968255</td>\n",
       "      <td>0.619944</td>\n",
       "      <td>0.885138</td>\n",
       "      <td>0.961690</td>\n",
       "      <td>0.803573</td>\n",
       "      <td>0.913771</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41183.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0.749048</td>\n",
       "      <td>0.760121</td>\n",
       "      <td>0.185418</td>\n",
       "      <td>0.749048</td>\n",
       "      <td>0.551045</td>\n",
       "      <td>0.185842</td>\n",
       "      <td>0.665388</td>\n",
       "      <td>0.053544</td>\n",
       "      <td>0.335893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18649.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0.786586</td>\n",
       "      <td>0.574780</td>\n",
       "      <td>0.662055</td>\n",
       "      <td>0.786586</td>\n",
       "      <td>0.598732</td>\n",
       "      <td>0.662323</td>\n",
       "      <td>0.513326</td>\n",
       "      <td>0.257391</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28590.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0.715295</td>\n",
       "      <td>0.799239</td>\n",
       "      <td>0.986412</td>\n",
       "      <td>0.715295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996683</td>\n",
       "      <td>0.965851</td>\n",
       "      <td>0.968420</td>\n",
       "      <td>0.878464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>05236.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0.733876</td>\n",
       "      <td>0.611239</td>\n",
       "      <td>0.999522</td>\n",
       "      <td>0.733876</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>0.742666</td>\n",
       "      <td>0.171123</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35323.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0.796713</td>\n",
       "      <td>0.807293</td>\n",
       "      <td>0.916658</td>\n",
       "      <td>0.796712</td>\n",
       "      <td>0.766862</td>\n",
       "      <td>0.847347</td>\n",
       "      <td>0.888179</td>\n",
       "      <td>0.982916</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10173.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>0.894867</td>\n",
       "      <td>0.641229</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.894867</td>\n",
       "      <td>0.999445</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.629070</td>\n",
       "      <td>0.131945</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31981.png</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.519866</td>\n",
       "      <td>0.489435</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490245</td>\n",
       "      <td>0.782317</td>\n",
       "      <td>0.034864</td>\n",
       "      <td>0.399122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62066.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0.504564</td>\n",
       "      <td>0.818432</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.504564</td>\n",
       "      <td>0.982027</td>\n",
       "      <td>0.984479</td>\n",
       "      <td>0.814675</td>\n",
       "      <td>0.974759</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36276.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0.599169</td>\n",
       "      <td>0.428866</td>\n",
       "      <td>0.911790</td>\n",
       "      <td>0.599169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911790</td>\n",
       "      <td>0.747988</td>\n",
       "      <td>0.502061</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23824.png</td>\n",
       "      <td>4</td>\n",
       "      <td>0.936679</td>\n",
       "      <td>0.552980</td>\n",
       "      <td>0.777544</td>\n",
       "      <td>0.936679</td>\n",
       "      <td>0.656237</td>\n",
       "      <td>0.777544</td>\n",
       "      <td>0.673958</td>\n",
       "      <td>0.017242</td>\n",
       "      <td>0.937461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45974.png</td>\n",
       "      <td>4</td>\n",
       "      <td>0.606763</td>\n",
       "      <td>0.727767</td>\n",
       "      <td>0.515833</td>\n",
       "      <td>0.606763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.508900</td>\n",
       "      <td>0.784639</td>\n",
       "      <td>0.424040</td>\n",
       "      <td>0.722465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>63003.png</td>\n",
       "      <td>4</td>\n",
       "      <td>0.958695</td>\n",
       "      <td>0.881221</td>\n",
       "      <td>0.073532</td>\n",
       "      <td>0.958695</td>\n",
       "      <td>0.594391</td>\n",
       "      <td>0.076981</td>\n",
       "      <td>0.810695</td>\n",
       "      <td>0.060772</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>02115.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0.634794</td>\n",
       "      <td>0.307666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.634794</td>\n",
       "      <td>0.403557</td>\n",
       "      <td>0.959241</td>\n",
       "      <td>0.743450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.511510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>61031.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>0.637043</td>\n",
       "      <td>0.896910</td>\n",
       "      <td>0.073624</td>\n",
       "      <td>0.637043</td>\n",
       "      <td>0.792571</td>\n",
       "      <td>0.073668</td>\n",
       "      <td>0.767869</td>\n",
       "      <td>0.991183</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38589.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557301</td>\n",
       "      <td>0.794620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.557301</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992055</td>\n",
       "      <td>0.201214</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46412.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754477</td>\n",
       "      <td>0.800807</td>\n",
       "      <td>0.407497</td>\n",
       "      <td>0.754477</td>\n",
       "      <td>0.587143</td>\n",
       "      <td>0.407497</td>\n",
       "      <td>0.816258</td>\n",
       "      <td>0.537338</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04012.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0.498139</td>\n",
       "      <td>0.474188</td>\n",
       "      <td>0.466365</td>\n",
       "      <td>0.498139</td>\n",
       "      <td>0.398059</td>\n",
       "      <td>0.466284</td>\n",
       "      <td>0.690475</td>\n",
       "      <td>0.312977</td>\n",
       "      <td>0.997711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70068.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602647</td>\n",
       "      <td>0.133648</td>\n",
       "      <td>0.073733</td>\n",
       "      <td>0.602647</td>\n",
       "      <td>0.752127</td>\n",
       "      <td>0.073733</td>\n",
       "      <td>0.660368</td>\n",
       "      <td>0.156302</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30648.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0.672663</td>\n",
       "      <td>0.411537</td>\n",
       "      <td>0.304122</td>\n",
       "      <td>0.672663</td>\n",
       "      <td>0.802191</td>\n",
       "      <td>0.304122</td>\n",
       "      <td>0.625713</td>\n",
       "      <td>0.060517</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53454.png</td>\n",
       "      <td>4</td>\n",
       "      <td>0.799619</td>\n",
       "      <td>0.506217</td>\n",
       "      <td>0.365146</td>\n",
       "      <td>0.799619</td>\n",
       "      <td>0.518505</td>\n",
       "      <td>0.365150</td>\n",
       "      <td>0.503965</td>\n",
       "      <td>0.275932</td>\n",
       "      <td>0.990583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>68289.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>0.494862</td>\n",
       "      <td>0.592081</td>\n",
       "      <td>0.563539</td>\n",
       "      <td>0.494862</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.563623</td>\n",
       "      <td>0.786480</td>\n",
       "      <td>0.967470</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44925.png</td>\n",
       "      <td>4</td>\n",
       "      <td>0.955766</td>\n",
       "      <td>0.426915</td>\n",
       "      <td>0.863794</td>\n",
       "      <td>0.955766</td>\n",
       "      <td>0.682785</td>\n",
       "      <td>0.863297</td>\n",
       "      <td>0.523760</td>\n",
       "      <td>0.263166</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  Truth  Prediction  FGSM_prediction  BIM_prediction  \\\n",
       "0  18666.jpg      3    0.891935         0.921653        0.873296   \n",
       "1  60731.jpg      1    0.619944         0.204064        0.968255   \n",
       "2  41183.jpg      3    0.749048         0.760121        0.185418   \n",
       "3  18649.jpg      3    0.786586         0.574780        0.662055   \n",
       "4  28590.jpg      0    0.715295         0.799239        0.986412   \n",
       "5  05236.jpg      3    0.733876         0.611239        0.999522   \n",
       "6  35323.jpg      3    0.796713         0.807293        0.916658   \n",
       "7  10173.jpg      2    0.894867         0.641229        0.999997   \n",
       "0  31981.png      4    1.000000         0.519866        0.489435   \n",
       "1  62066.jpg      0    0.504564         0.818432        0.999976   \n",
       "2  36276.jpg      3    0.599169         0.428866        0.911790   \n",
       "3  23824.png      4    0.936679         0.552980        0.777544   \n",
       "4  45974.png      4    0.606763         0.727767        0.515833   \n",
       "5  63003.png      4    0.958695         0.881221        0.073532   \n",
       "6  02115.jpg      0    0.634794         0.307666        1.000000   \n",
       "7  61031.jpg      2    0.637043         0.896910        0.073624   \n",
       "0  38589.jpg      0    0.557301         0.794620        1.000000   \n",
       "1  46412.jpg      3    0.754477         0.800807        0.407497   \n",
       "2  04012.jpg      3    0.498139         0.474188        0.466365   \n",
       "3  70068.jpg      1    0.602647         0.133648        0.073733   \n",
       "4  30648.jpg      3    0.672663         0.411537        0.304122   \n",
       "5  53454.png      4    0.799619         0.506217        0.365146   \n",
       "6  68289.jpg      2    0.494862         0.592081        0.563539   \n",
       "7  44925.png      4    0.955766         0.426915        0.863794   \n",
       "\n",
       "   CW_prediction  RFGSM_prediction  PGD_prediction  FFGSM_prediction  \\\n",
       "0       0.891935          0.759984        0.873296          0.553913   \n",
       "1       0.619944          0.885138        0.961690          0.803573   \n",
       "2       0.749048          0.551045        0.185842          0.665388   \n",
       "3       0.786586          0.598732        0.662323          0.513326   \n",
       "4       0.715295          1.000000        0.996683          0.965851   \n",
       "5       0.733876          1.000000        0.999705          0.742666   \n",
       "6       0.796712          0.766862        0.847347          0.888179   \n",
       "7       0.894867          0.999445        0.999997          0.629070   \n",
       "0       1.000000          1.000000        0.490245          0.782317   \n",
       "1       0.504564          0.982027        0.984479          0.814675   \n",
       "2       0.599169          1.000000        0.911790          0.747988   \n",
       "3       0.936679          0.656237        0.777544          0.673958   \n",
       "4       0.606763          1.000000        0.508900          0.784639   \n",
       "5       0.958695          0.594391        0.076981          0.810695   \n",
       "6       0.634794          0.403557        0.959241          0.743450   \n",
       "7       0.637043          0.792571        0.073668          0.767869   \n",
       "0       0.557301          1.000000        0.992055          0.201214   \n",
       "1       0.754477          0.587143        0.407497          0.816258   \n",
       "2       0.498139          0.398059        0.466284          0.690475   \n",
       "3       0.602647          0.752127        0.073733          0.660368   \n",
       "4       0.672663          0.802191        0.304122          0.625713   \n",
       "5       0.799619          0.518505        0.365150          0.503965   \n",
       "6       0.494862          1.000000        0.563623          0.786480   \n",
       "7       0.955766          0.682785        0.863297          0.523760   \n",
       "\n",
       "   MIFGSM_prediction  TPGD_prediction  \n",
       "0           0.474206         1.000000  \n",
       "1           0.913771         0.999998  \n",
       "2           0.053544         0.335893  \n",
       "3           0.257391         1.000000  \n",
       "4           0.968420         0.878464  \n",
       "5           0.171123         1.000000  \n",
       "6           0.982916         1.000000  \n",
       "7           0.131945         1.000000  \n",
       "0           0.034864         0.399122  \n",
       "1           0.974759         1.000000  \n",
       "2           0.502061         1.000000  \n",
       "3           0.017242         0.937461  \n",
       "4           0.424040         0.722465  \n",
       "5           0.060772         1.000000  \n",
       "6           1.000000         0.511510  \n",
       "7           0.991183         1.000000  \n",
       "0           0.667700         1.000000  \n",
       "1           0.537338         1.000000  \n",
       "2           0.312977         0.997711  \n",
       "3           0.156302         1.000000  \n",
       "4           0.060517         1.000000  \n",
       "5           0.275932         0.990583  \n",
       "6           0.967470         1.000000  \n",
       "7           0.263166         1.000000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning\n",
    "    All algorithms: Juniward, ., ., steganogan\n",
    "    All adv attks......\n",
    "    1. Create Stego Images\n",
    "    2. Get predictions for all algorithms\n",
    "    3. All Adv. Attack for all algorithms \n",
    "    4. Fine Tuning Kaggle Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, (targets==0).long()*np.random.randint(1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save SteganoGAN adv atk images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGSM\n",
      "BIM\n",
      "FGSM\n",
      "BIM\n",
      "FGSM\n",
      "BIM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Id': ['50618.png', '15643.png'],\n",
       "  'Truth': [4, 4],\n",
       "  'Prediction': [0.73895174, 0.7498165],\n",
       "  'FGSM_prediction': array([0.7456739, 0.7600925], dtype=float32),\n",
       "  'BIM_prediction': array([0.74869156, 0.7362946 ], dtype=float32)},\n",
       " {'Id': ['41275.png', '24378.png'],\n",
       "  'Truth': [4, 4],\n",
       "  'Prediction': [0.7449106, 0.7380177],\n",
       "  'FGSM_prediction': array([0.73248255, 0.74366957], dtype=float32),\n",
       "  'BIM_prediction': array([0.757368  , 0.76377225], dtype=float32)},\n",
       " {'Id': ['05763.png', '62324.png'],\n",
       "  'Truth': [4, 4],\n",
       "  'Prediction': [0.7391696, 0.74663615],\n",
       "  'FGSM_prediction': array([0.76294565, 0.7247    ], dtype=float32),\n",
       "  'BIM_prediction': array([0.74261284, 0.7545247 ], dtype=float32)}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@ Tanveer\n",
    "import torchattacks\n",
    "atks = [torchattacks.FGSM(net, eps=8/255),\n",
    "        torchattacks.BIM(net, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.CW(net, c=1, kappa=0, steps=1000, lr=0.01),\n",
    "        torchattacks.RFGSM(net, eps=8/255, alpha=4/255, steps=1),\n",
    "        torchattacks.PGD(net, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.FFGSM(net, eps=8/255, alpha=12/255),\n",
    "        torchattacks.MIFGSM(net, eps=8/255, decay=1.0, steps=5),\n",
    "        torchattacks.TPGD(net, eps=8/255, alpha=2/255, steps=7),\n",
    "       ]\n",
    "\n",
    "val_dataset = CustomDatasetRetriever(\n",
    "    kinds=dataset[dataset['fold'] == fold_number].kind.values,\n",
    "    image_names=dataset[dataset['fold'] == fold_number].image_name.values,\n",
    "    labels=dataset[dataset['fold'] == fold_number].label.values,\n",
    "    transforms=get_valid_transforms(),\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "target_map_function = lambda images, labels: (labels==0).long()*np.random.randint(1,4)\n",
    "for atk in atks:\n",
    "    if atk.attack != 'TPGD':\n",
    "        atk.set_targeted_mode(target_map_function=target_map_function)\n",
    "    \n",
    "\n",
    "for step, (image_names, images, targets) in enumerate(data_loader):\n",
    "    print(step, end='\\r')\n",
    "    start = time.time()\n",
    "    targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "    for atk in atks :\n",
    "        print(atk.attack)\n",
    "        if atk.attack != 'TPGD':\n",
    "             adv_images = atk(images, targets)\n",
    "        else:\n",
    "             adv_images = atk(images, (targets==0).long()*np.random.randint(1,4))\n",
    "        names = (list(map(lambda x: f'{atk.attack}_' + x , image_names)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "names = (list(map(lambda x: x + f'_{atk.attack}' , image_names)))\n",
    "for name, img in zip(names, adv_images):\n",
    "    save_image(img, f'alaska2-image-steganalysis/SteganoGAN_Attk/{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512, 512])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['05763_BIM.png', '62324_BIM.png']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list(map(lambda x: x.split('.')[0] + f'_{atk.attack}.' + x.split('.')[1] , image_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/hannan/alaska\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
